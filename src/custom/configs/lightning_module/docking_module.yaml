_target_: custom.model.lightning_module.DockingModel

# Architecture is loaded via defaults: /lightning_module/architecture: dockingdit_100M
# You can override specific architecture parameters here if needed:
# architecture:
#   some_param: value

mint_model:
  _target_: mint.model.esm.ESM2
  num_layers: 33
  embed_dim: 1280
  attention_heads: 20
  token_dropout: False
  use_multimer: True
  activation_checkpointing: False

mint_use_weights: mint # esm2 / mint / null (no weights)
mint_layers: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]

multiplicity: 48 # Number of dimer copies per GPU for efficient MC averaging over t,R,z.

scale_true_coords: 16.0 # True coords are divided by this factor before being passed to the model.
scale_ref_coords: 5.0 # Ref coords are divided by this factor before being passed to the model.

interpolant:
  _target_: custom.model.interpolants.LinearInterpolant

optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 1e-4
  weight_decay: 0.0

scheduler:
  _target_: simplefold.utils.lr_scheduler.LinearWarmup
  _partial_: true
  min_lr: 1e-6
  max_lr: ${..optimizer.lr}
  warmup_steps: 5000

sampler:
  _target_: custom.model.sampling.EulerSampler
  _partial_: true
  num_timesteps: 500
  t_start: 1e-4
  t_end: 1.0

backbone_only: True # Whether the training data is backbone only.
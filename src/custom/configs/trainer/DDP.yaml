_target_: lightning.pytorch.trainer.Trainer
default_root_dir: ./artifacts

deterministic: False

accelerator: gpu
devices: auto
strategy: 
  _target_: lightning.pytorch.strategies.DDPStrategy
  find_unused_parameters: True
  gradient_as_bucket_view: True
num_nodes: 1
precision: bf16-mixed
accumulate_grad_batches: 4
max_steps: 100000
logger:
  - _target_: lightning.pytorch.loggers.WandbLogger
    project: docking_model
    # name: ${hydra:job.name}
    save_dir: ${trainer.default_root_dir}/logger
    log_model: False


num_sanity_val_steps: 0 # Num of sanity check validation_steps to run before training starts


# Validation and checkpointing:
check_val_every_n_epoch: null
val_check_interval: 500 # counts number of training batches
enable_checkpointing: True
callbacks:
  - _target_: lightning.pytorch.callbacks.ModelCheckpoint
    dirpath: ${trainer.default_root_dir}/checkpoints
    filename: "best_val_loss-{step}" # optimizer / global step in filename
    monitor: "val/loss"
    mode: "min"
    save_top_k: 1
    save_last: False
    save_on_train_epoch_end: False
    auto_insert_metric_name: False

# Checkpointing every n steps:
  - _target_: lightning.pytorch.callbacks.ModelCheckpoint
    dirpath: ${trainer.default_root_dir}/checkpoints
    filename: "periodic-{step}" # optimizer / global step in filename
    monitor: "trainer/global_step" # optimizer steps
    mode: "max"
    save_top_k: 1
    save_last: True
    every_n_train_steps: 1000 # global steps
    save_on_train_epoch_end: False
    auto_insert_metric_name: False



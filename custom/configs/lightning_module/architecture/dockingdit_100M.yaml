_target_: custom.model.architecture.DockingDiT

# Model dimensions:
d_a: 768
d_e: ${..esm_d_e}
pLM_num_layers: ${..esm_num_layers}
pLM_dropout_prob: 0.0
use_length_condition: True

# Encoder configuration:
encoder:
  _target_: custom.model.architecture.Encoder
  d_a: 768
  atom_n_queries_enc: 32
  atom_n_keys_enc: 128
  atom_encoder_transformer:
    _target_: model.torch.blocks.HomogenTrunk
    depth: 1
    block:
      _target_: model.torch.blocks.DiTBlock
      _partial_: True
      hidden_size: 256
      mlp_ratio: 4.0
      use_swiglu: True
      self_attention_layer:
        _target_: model.torch.layers.EfficientSelfAttentionLayer
        _partial_: True
        hidden_size: 256
        num_heads: 4
        qk_norm: True
        pos_embedder:
          _target_: model.torch.pos_embed.AxialRotaryPositionEncoding
          in_dim: 4
          embed_dim: 256
          num_heads: 4
          base: 100.0

# Decoder configuration:
decoder:
  _target_: custom.model.architecture.Decoder
  d_a: 768
  D: 256  # atom_hidden_size_dec
  atom_n_queries_dec: 32
  atom_n_keys_dec: 128
  atom_decoder_transformer:
    _target_: model.torch.blocks.HomogenTrunk
    depth: 1
    block:
      _target_: model.torch.blocks.DiTBlock
      _partial_: True
      hidden_size: 256
      mlp_ratio: 4.0
      use_swiglu: True
      self_attention_layer:
        _target_: model.torch.layers.EfficientSelfAttentionLayer
        _partial_: True
        hidden_size: 256
        num_heads: 4
        qk_norm: True
        pos_embedder:
          _target_: model.torch.pos_embed.AxialRotaryPositionEncoding
          in_dim: 4
          embed_dim: 256
          num_heads: 4
          base: 100.0

# Main trunk (multimer trunk) configuration:
mtm_res_trunk:
  _target_: model.torch.blocks.HomogenTrunk
  depth: 8
  block:
    _target_: model.torch.blocks.DiTBlock
    _partial_: True
    hidden_size: 768
    mlp_ratio: 4.0
    use_swiglu: True
    self_attention_layer:
      _target_: model.torch.layers.EfficientSelfAttentionLayer
      _partial_: True
      hidden_size: 768
      num_heads: 12
      qk_norm: True
      pos_embedder:
        _target_: model.torch.pos_embed.AxialRotaryPositionEncoding
        in_dim: 1
        embed_dim: 768
        num_heads: 12
        base: 100.0
